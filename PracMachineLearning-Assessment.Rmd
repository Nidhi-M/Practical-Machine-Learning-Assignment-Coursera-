---
title: "Practical Machine Learning - Peer Assessment"
author: "Nidhi Mavani"
date: "Saturday, December 20, 2014"
output: html_document
---

###An Analysis of the Weight Lifting Exercises Dataset

This document presents the results of the Practical Machine Learning Peer Assessments in a report using a single R markdown document that can be processed by knitr and be transformed into an HTML file.     

####Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement Â- a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks.  
One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this data set, the participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website [here](http://groupware.les.inf.puc-rio.br/har) (see the section on the Weight Lifting Exercise Dataset).  
In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants toto predict the manner in which praticipants did the exercise.  
Since we have a data set with too many columns and we need make a class prediction, therefore decided to implement a random forests model, that's no need cross-validation or a separate test set to get an unbiased estimate of the test set error.  
  
The dependent variable or response is the "classe" variable in the training set.
  
####Data
#####Getting And Cleaning of Data
Load the data.

```{r cache=TRUE, warning=FALSE,message=FALSE}
training <- read.csv("pml-training.csv", header=T, sep=",", na.strings=c("NA",""))
testing<-read.csv("pml-testing.csv", header=T,sep=",", na.strings=c("NA",""))
dim(training)
dim(testing)
```

#####Data Exploration 
Firstly Load the Libraries and Set Seed for results to be Reproducible.
```{r warning=FALSE,message=FALSE}
library(caret)
library(randomForest)
set.seed(45)
```
The following approaches for used for reducing the number of predictors.

- <span style="color:green">Remove variables that have too many NA values.</span>  
As there are many variables that are of value NA we remove those columns that have 70% of NAs,inorder to find out which Columns satisfy the criteria  
  
  
```{r}
##this will return the total number of Non NAs values in each Column
ColSums<-colSums(!is.na(training[,-ncol(training)]))
head(ColSums)
## this gives the number of valid columns
sum(colSums(!is.na(training[,-ncol(training)]))>=0.7*nrow(training)) 

```
  
Subset the Data by considering only valid columns
```{r}
validCol<-colSums(!is.na(training[,-ncol(training)]))>=0.7*nrow(training)
trainingWithlessNA<-training[,validCol]
dim(trainingWithlessNA)
```
  
- <span style="color:green">Remove columns having near zero variance</span>
  
```{r cache=TRUE}
nzv <- nearZeroVar(trainingWithlessNA, saveMetrics = TRUE) ##this creates a matrix
head(nzv)
#this will return only those col numbers
nzv <- nearZeroVar(trainingWithlessNA)
trainingWithNozeroVar <- trainingWithlessNA[, -nzv]
dim(trainingWithNozeroVar)

```

- <span style="color:green">Remove the ID Variable</span>
```{r}
trainingWithNozeroVarandID<-trainingWithNozeroVar[,-1]
dim(trainingWithNozeroVarandID)
```

- <span style="color:green">Remove those variable having high correlation</span>  

```{r}
##number of the columns with numeric values
sum(sapply(trainingWithNozeroVarandID, is.numeric))
##VAriables with high correraltion
corrMatrix <- 
  cor(na.omit(trainingWithNozeroVarandID[sapply(trainingWithNozeroVarandID, is.numeric)]))
dim(corrMatrix)
```
```{r warning=FALSE,message=FALSE, results='hide'}
##A cut-off of 90% is kept 
highCorr<-findCorrelation(corrMatrix, cutoff = .90, verbose = T)
## Final Training set 
Final_training<-trainingWithNozeroVarandID[,-highCorr]
```
```{r}
dim(Final_training)
```

<span style="color:blue">**The same is to be done with testing Data**</span>  
```{r}
testingWithLessNA<-testing[,validCol]
testingWithNoZeroVar<-testingWithLessNA[,-nzv]
testingWithNoZeroVarAndID<-testingWithNoZeroVar[,-1]
Final_testing<-testingWithNoZeroVarAndID[,-highCorr]
dim(Final_testing)
```

####Analysis

#####Partitioning the training Data

```{r}
set.seed(45)
inTrain<-createDataPartition(Final_training$classe,p=0.75,list=F)
modelTrain<-Final_training[inTrain,]
modeltest<-Final_training[-inTrain,]
```

#####Model Fitting
Random forests build lots of bushy trees, and then average them to reduce the variance.  
In random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the execution. So, we proced with the training the model (Random Forest) with the training data set.

```{r cache=TRUE}
set.seed(45)
require(randomForest)
modelFit<-randomForest(classe~.,data=Final_training,importance=TRUE)
modelFit
```
No. of variables tried at each split: 7. It means every time we only randomly use 7 predictors to grow the tree.
```{r}
varImpPlot(modelFit)
```


###Out-of Sample Accuracy
Now lets evaluate this tree on the testing data.    
Accuracy is 1 (100%)
```{r}
confusionMatrix(predict(modelFit,newdata=modeltest[,-ncol(modeltest)]),
                modeltest$classe)
```

###Prediction

Now lets Predict for the Final_testing Data
```{r}
predictions <- predict(modelFit,newdata=Final_testing)
predictions
```
Those answers were submitted as a part of Course and are Correct